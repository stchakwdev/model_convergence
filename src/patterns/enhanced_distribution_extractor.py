"""
Enhanced Distribution Extractor for Tilli Tonse Framework

Optimized for extracting meaningful probability distributions from the rich,
200-500 token responses generated by story-based prompts with checkpoints.
"""

import re
import torch
import numpy as np
from typing import Dict, List, Tuple, Any
from collections import Counter
import torch.nn.functional as F


class TilliTonseDistributionExtractor:
    """
    Distribution extractor optimized for multi-turn story responses.
    
    Key improvements for longer sequences:
    1. Larger vocabulary (2000-3000 tokens)
    2. N-gram analysis for phrase patterns
    3. Semantic clustering of related tokens
    4. Checkpoint-specific distribution analysis
    """
    
    def __init__(self, 
                 common_vocab_size: int = 2500,
                 include_ngrams: bool = True,
                 ngram_max_length: int = 3):
        self.common_vocab_size = common_vocab_size
        self.include_ngrams = include_ngrams
        self.ngram_max_length = ngram_max_length
        self.vocabulary = None
        self.token_to_id = {}
        self.ngram_vocabulary = None
        self.ngram_to_id = {}
        
    def extract_distributions_from_story_responses(self, 
                                                 grouped_responses: Dict[str, List[str]]) -> Dict[str, torch.Tensor]:
        """
        Extract rich probability distributions from Tilli Tonse story responses.
        
        Args:
            grouped_responses: {model_name: [full_story_response1, ...]}
        Returns:
            Dict mapping model names to distribution tensors
        """
        
        print("🎭 Extracting distributions from Tilli Tonse story responses...")
        print(f"   Target vocabulary size: {self.common_vocab_size}")
        
        # First, analyze response lengths to confirm improvement
        self._analyze_response_richness(grouped_responses)
        
        # Build enhanced vocabulary from all responses
        all_tokens, all_ngrams = self._build_enhanced_vocabulary(grouped_responses)
        
        # Create unified vocabulary (tokens + important n-grams)
        self._create_unified_vocabulary(all_tokens, all_ngrams)
        
        # Extract distributions for each model
        model_distributions = {}
        
        for model_name, responses in grouped_responses.items():
            print(f"  🤖 Processing {model_name}: {len(responses)} story responses")
            
            distributions = []
            for response in responses:
                # Extract distribution from this rich story response
                dist = self._estimate_distribution_from_story(response)
                distributions.append(dist)
            
            # Stack into tensor: (n_responses, vocab_size)
            model_distributions[model_name] = torch.stack(distributions)
            
            avg_response_length = np.mean([len(r.split()) for r in responses])
            print(f"     Average response length: {avg_response_length:.0f} tokens")
        
        return model_distributions
    
    def _analyze_response_richness(self, grouped_responses: Dict[str, List[str]]):
        """Analyze how much richer the Tilli Tonse responses are"""
        
        total_tokens = 0
        total_responses = 0
        min_length = float('inf')
        max_length = 0
        
        for model_responses in grouped_responses.values():
            for response in model_responses:
                length = len(response.split())
                total_tokens += length
                total_responses += 1
                min_length = min(min_length, length)
                max_length = max(max_length, length)
        
        avg_length = total_tokens / total_responses if total_responses > 0 else 0
        
        print(f"📊 Response richness analysis:")
        print(f"   Average tokens per response: {avg_length:.1f}")
        print(f"   Range: {min_length} - {max_length} tokens")
        print(f"   Total tokens: {total_tokens:,}")
        print(f"   Improvement: {avg_length/5:.1f}x richer than simple Q&A (≈5 tokens)")
    
    def _build_enhanced_vocabulary(self, grouped_responses: Dict[str, List[str]]) -> Tuple[List[str], List[str]]:
        """Build enhanced vocabulary with tokens and n-grams"""
        
        all_tokens = []
        all_ngrams = []
        
        for model_responses in grouped_responses.values():
            for response in model_responses:
                # Extract tokens
                tokens = self._tokenize_story_response(response)
                all_tokens.extend(tokens)
                
                # Extract n-grams for phrase patterns
                if self.include_ngrams:
                    ngrams = self._extract_ngrams(tokens)
                    all_ngrams.extend(ngrams)
        
        print(f"   📚 Extracted {len(all_tokens)} tokens, {len(all_ngrams)} n-grams")
        return all_tokens, all_ngrams
    
    def _tokenize_story_response(self, response: str) -> List[str]:
        """Enhanced tokenization for story responses"""
        
        # Clean text
        response = response.lower().strip()
        
        # Handle common story elements
        response = re.sub(r'\btilli tonse\b', '<tilli_tonse>', response)  # Preserve cultural marker
        response = re.sub(r'\bmoral|lesson\b', '<moral_keyword>', response)  # Mark moral extraction
        response = re.sub(r'\bstory|tale\b', '<story_keyword>', response)  # Mark narrative elements
        
        # Tokenize on word boundaries and punctuation
        tokens = re.findall(r'\w+|[^\w\s]', response)
        
        # Filter and enhance
        filtered_tokens = ['<start>']  # Start marker
        
        for token in tokens:
            if len(token) >= 1:
                # Preserve special markers
                if token.startswith('<') and token.endswith('>'):
                    filtered_tokens.append(token)
                # Regular words
                elif token.isalpha() and len(token) >= 2:
                    filtered_tokens.append(token)
                # Numbers and important punctuation
                elif token.isdigit() or token in ['.', '!', '?', ',', ':', ';']:
                    filtered_tokens.append(token)
        
        filtered_tokens.append('<end>')  # End marker
        return filtered_tokens
    
    def _extract_ngrams(self, tokens: List[str]) -> List[str]:
        """Extract meaningful n-grams from tokens"""
        
        ngrams = []
        
        for n in range(2, min(self.ngram_max_length + 1, len(tokens))):
            for i in range(len(tokens) - n + 1):
                ngram = ' '.join(tokens[i:i+n])
                
                # Only keep ngrams with meaningful content (no all-punctuation)
                if any(token.isalpha() for token in tokens[i:i+n]):
                    ngrams.append(f"<ngram_{n}>_{ngram}")
        
        return ngrams
    
    def _create_unified_vocabulary(self, all_tokens: List[str], all_ngrams: List[str]):
        """Create unified vocabulary with both tokens and important n-grams"""
        
        # Count token frequencies
        token_counts = Counter(all_tokens)
        ngram_counts = Counter(all_ngrams)
        
        # Select most common tokens (80% of vocabulary)
        token_vocab_size = int(self.common_vocab_size * 0.8)
        most_common_tokens = token_counts.most_common(token_vocab_size)
        
        # Select most common n-grams (20% of vocabulary)
        ngram_vocab_size = self.common_vocab_size - token_vocab_size
        most_common_ngrams = ngram_counts.most_common(ngram_vocab_size)
        
        # Combine vocabularies
        self.vocabulary = []
        
        # Add tokens
        for token, count in most_common_tokens:
            self.vocabulary.append(token)
        
        # Add n-grams
        for ngram, count in most_common_ngrams:
            self.vocabulary.append(ngram)
        
        # Create lookup mapping
        self.token_to_id = {token: i for i, token in enumerate(self.vocabulary)}
        
        print(f"   📖 Built unified vocabulary:")
        print(f"      Tokens: {len(most_common_tokens)}")
        print(f"      N-grams: {len(most_common_ngrams)}")
        print(f"      Total: {len(self.vocabulary)}")
    
    def _estimate_distribution_from_story(self, response: str) -> torch.Tensor:
        """
        Estimate probability distribution from a rich story response.
        
        This leverages the much longer sequences (200-500 tokens) to create
        meaningful probability distributions for KL divergence analysis.
        """
        
        # Tokenize the story response
        tokens = self._tokenize_story_response(response)
        
        # Extract n-grams if enabled
        ngrams = []
        if self.include_ngrams:
            ngrams = self._extract_ngrams(tokens)
        
        # Count occurrences in vocabulary
        token_counts = np.zeros(len(self.vocabulary))
        
        # Count tokens
        for token in tokens:
            if token in self.token_to_id:
                token_counts[self.token_to_id[token]] += 1
        
        # Count n-grams
        for ngram in ngrams:
            if ngram in self.token_to_id:
                token_counts[self.token_to_id[ngram]] += 1
        
        # Convert to probability distribution
        if np.sum(token_counts) > 0:
            probabilities = token_counts / np.sum(token_counts)
        else:
            # Fallback uniform distribution
            probabilities = np.ones(len(self.vocabulary)) / len(self.vocabulary)
        
        # Add epsilon smoothing for numerical stability
        epsilon = 1e-8
        probabilities = (probabilities + epsilon) / np.sum(probabilities + epsilon)
        
        return torch.from_numpy(probabilities).float()
    
    def analyze_checkpoint_distributions(self, 
                                       checkpoint_responses: Dict[str, Dict[str, List[str]]]) -> Dict[str, Dict[str, torch.Tensor]]:
        """
        Analyze distributions for specific checkpoint types.
        
        Args:
            checkpoint_responses: {model_name: {checkpoint_type: [responses]}}
        Returns:
            {checkpoint_type: {model_name: distribution_tensor}}
        """
        
        print("🔍 Analyzing checkpoint-specific distributions...")
        
        checkpoint_distributions = {}
        
        # Get all checkpoint types
        all_checkpoint_types = set()
        for model_data in checkpoint_responses.values():
            all_checkpoint_types.update(model_data.keys())
        
        for checkpoint_type in all_checkpoint_types:
            print(f"   📍 Processing {checkpoint_type} checkpoints")
            
            # Gather responses for this checkpoint type
            type_responses = {}
            for model_name, model_data in checkpoint_responses.items():
                if checkpoint_type in model_data:
                    type_responses[model_name] = model_data[checkpoint_type]
            
            if len(type_responses) >= 2:  # Need at least 2 models to compare
                # Extract distributions for this checkpoint type
                type_distributions = self.extract_distributions_from_story_responses(type_responses)
                checkpoint_distributions[checkpoint_type] = type_distributions
        
        return checkpoint_distributions
    
    def get_vocabulary_stats(self) -> Dict[str, Any]:
        """Get statistics about the built vocabulary"""
        
        if not self.vocabulary:
            return {"error": "Vocabulary not built yet"}
        
        token_count = sum(1 for item in self.vocabulary if not item.startswith('<ngram_'))
        ngram_count = sum(1 for item in self.vocabulary if item.startswith('<ngram_'))
        
        return {
            "total_vocabulary_size": len(self.vocabulary),
            "token_count": token_count,
            "ngram_count": ngram_count,
            "includes_cultural_markers": any('<tilli_tonse>' in item for item in self.vocabulary),
            "includes_moral_markers": any('<moral_keyword>' in item for item in self.vocabulary),
            "vocabulary_richness": len(self.vocabulary) / 1000  # Relative to standard 1k vocab
        }